{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "base_path = os.getenv(\"LSM_BASE\")\n",
    "if not base_path:\n",
    "    raise ValueError(\"‚ùå Environment variable 'LSM_BASE' is not set!\")\n",
    "\n",
    "model_path         = os.path.join(base_path, \"models\", \"hand_landmarker.task\")\n",
    "raw_data_path      = os.path.join(base_path, \"data\", \"raw\")\n",
    "# static_json_path   = os.path.join(base_path, \"data\", \"metadata\", \"static_gestures.json\")\n",
    "# dynamic_json_path  = os.path.join(base_path, \"data\", \"metadata\", \"dynamic_gestures.json\")\n",
    "\n",
    "# 3) Gather all image files under data/raw/<letter>/\n",
    "# image_records list is in lexicographical order by folder name, then lexicographical order by file name\n",
    "valid_exts = {\".jpg\", \".jpeg\", \".png\"}\n",
    "image_records = []  # list of (letter, path)\n",
    "\n",
    "for letter_dir in sorted(os.listdir(raw_data_path)):\n",
    "    letter_path = os.path.join(raw_data_path, letter_dir)\n",
    "    if not os.path.isdir(letter_path):\n",
    "        continue\n",
    "    for file in sorted(os.listdir(letter_path)):\n",
    "        ext = Path(file).suffix.lower()\n",
    "        if ext in valid_exts:\n",
    "            image_records.append((letter_dir, os.path.join(letter_path, file)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Quick check\n",
    "print(f\"Found {len(image_records)} image files in {raw_data_path!r}\")\n",
    "print(\"First 10 records:\")\n",
    "for rec in image_records[:10]:\n",
    "    print(\" \", rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: üß™ Define preprocessing functions and test on one image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Preprocessing functions\n",
    "def adjust_lighting(image):\n",
    "    ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    ycrcb[:, :, 0] = cv2.equalizeHist(ycrcb[:, :, 0])\n",
    "    return cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "def gamma_correction(image, gamma=1.5):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([((i/255.0)**inv_gamma)*255 for i in np.arange(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def attenuate_red(image):\n",
    "    b, g, r = cv2.split(image)\n",
    "    r = (r * 0.3).astype(np.uint8)\n",
    "    return cv2.merge([b, g, r])\n",
    "\n",
    "def resize_and_pad(image, size=(640, 480)):\n",
    "    h, w = image.shape[:2]\n",
    "    scale = min(size[0]/w, size[1]/h)\n",
    "    new_w, new_h = int(w*scale), int(h*scale)\n",
    "    resized = cv2.resize(image, (new_w, new_h))\n",
    "    top = (size[1] - new_h) // 2\n",
    "    bottom = size[1] - new_h - top\n",
    "    left = (size[0] - new_w) // 2\n",
    "    right = size[0] - new_w - left\n",
    "    return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\n",
    "def full_preprocess(image):\n",
    "    img = adjust_lighting(image)\n",
    "    img = gamma_correction(img)\n",
    "    img = attenuate_red(img)\n",
    "    img = resize_and_pad(img)\n",
    "    return img\n",
    "\n",
    "# Load a sample image\n",
    "# Replace this with the first path from your image_records\n",
    "sample_letter, sample_path = image_records[0]\n",
    "orig = cv2.imread(sample_path)\n",
    "\n",
    "# Apply each step\n",
    "step1 = adjust_lighting(orig)\n",
    "step2 = gamma_correction(step1)\n",
    "step3 = attenuate_red(step2)\n",
    "step4 = resize_and_pad(step3)\n",
    "step_full = full_preprocess(orig)\n",
    "\n",
    "# Prepare for display (convert BGR -> RGB)\n",
    "images = [orig, step1, step2, step3, step4, step_full]\n",
    "titles = ['Original','Lighting','Gamma','Red Attenuated','Resized','Full Preprocess']\n",
    "images_rgb = [cv2.cvtColor(im, cv2.COLOR_BGR2RGB) for im in images]\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "for ax, title, im in zip(axes, titles, images_rgb):\n",
    "    ax.imshow(im)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21827cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.35\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MediaPipe Hands initialized:\", hands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b43950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from pandas import DataFrame\n",
    "\n",
    "# LSM: letters that REQUIRE motion\n",
    "DYNAMIC_LETTERS = {\"J\", \"K\", \"M\", \"Q\", \"X\", \"Z\"}\n",
    "\n",
    "# 1) Build one unified list\n",
    "all_entries = []\n",
    "\n",
    "for letter, img_path in image_records[:5]:   # or [:] for all images\n",
    "    img  = cv2.imread(img_path)\n",
    "    proc = resize_and_pad(img)                # from Cell 2\n",
    "\n",
    "    # Run detection\n",
    "    rgb    = cv2.cvtColor(proc, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    # Prepare handedness, landmarks, confidences\n",
    "    landmarks_dict  = {\"right_hand\": None, \"left_hand\": None}\n",
    "    confidence_dict = {\"right_hand\": None, \"left_hand\": None}\n",
    "    handedness_list = []\n",
    "\n",
    "    if result.multi_hand_landmarks and result.multi_handedness:\n",
    "        for lm_list, hand_h in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "            side = hand_h.classification[0].label.lower()  # \"right\" or \"left\"\n",
    "            handedness_list.append(side)\n",
    "            landmarks_dict[f\"{side}_hand\"] = [\n",
    "                {\"x\": lm.x, \"y\": lm.y, \"z\": lm.z}\n",
    "                for lm in lm_list.landmark\n",
    "            ]\n",
    "            confidence_dict[f\"{side}_hand\"] = hand_h.classification[0].score\n",
    "\n",
    "    # Compose the entry\n",
    "    entry = {\n",
    "        \"image\":                Path(img_path).name,\n",
    "        \"letter\":               letter,\n",
    "        \"gesture_type\":         \"dynamic\" if letter in DYNAMIC_LETTERS else \"static\",\n",
    "        \"timestamp\":            datetime.now(timezone.utc).isoformat(),\n",
    "        \"image_size\":           list(proc.shape[1::-1]),   # [width, height]\n",
    "        \"hand_count\":           len(handedness_list),\n",
    "        \"handedness_detected\":  handedness_list,\n",
    "        \"hand_confidence\":      confidence_dict,\n",
    "        \"landmarks\":            landmarks_dict\n",
    "    }\n",
    "    all_entries.append(entry)\n",
    "\n",
    "# 2) Preview in a DataFrame\n",
    "DataFrame(all_entries)\n",
    "\n",
    "# 3) Save to a single JSON file\n",
    "output_path = os.path.join(base_path, \"data\", \"metadata\", \"gestures.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_entries, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(all_entries)} entries to:\\n  {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
